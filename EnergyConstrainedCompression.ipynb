{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnergyConstrainedCompression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panda1230/pytorch-cnn-visualizations/blob/master/EnergyConstrainedCompression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiaeQP2rEpP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "from models import get_net_model\n",
        "from proj_utils import fill_model_weights, layers_stat, model_sparsity, filtered_parameters, \\\n",
        "    l0proj, round_model_weights, clamp_model_weights\n",
        "from sa_energy_model import build_energy_info, energy_eval2, energy_eval2_relax, energy_proj2, \\\n",
        "    reset_Xenergy_cache\n",
        "from utils import get_data_loaders, joint_loss, eval_loss_acc1_acc5, model_snapshot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl4qPE6lFFat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net='lenet-5'\n",
        "dataset = 'mnist-32'\n",
        "batch_size= 64\n",
        "val_batch_size=64\n",
        "num_workers =8 \n",
        "epochs =1\n",
        "lr_m = 0.001, #'learning rate'\n",
        "xlr =1e-4 #'learning rate for input mask'\n",
        "l2wd =1e-4 #'l2 weight decay'\n",
        "xl2wd=1e-5 #l2 weight decay (for input mask)'\n",
        "momentum =0.9 #momentum\n",
        "proj_int=10 #'how many batches for each projection'\n",
        "nodp = 1 #action='store_true', help='turn off dropout'\n",
        "input_mask=1 # action='store_true', help='enable input mask'\n",
        "randinit =1 #action='store_true', help='use random init'\n",
        "eval =1 #action='store_true', help='evaluate testset in the begining')\n",
        "#parser.add_argument('--pretrain', default=None, help='file to load pretrained model')\n",
        "#parser.add_argument('--eval', action='store_true', help='evaluate testset in the begining')\n",
        "#parser.add_argument('--seed', type=int, default=117, help='random seed')\n",
        "log_interval =100 # help='how many batches to wait before logging training status')\n",
        "test_interval = 1 # help='how many epochs to wait before another test')\n",
        "save_interval=10# help='how many epochs to wait before save a model')\n",
        "logdir = './sample_data/' #help='folder to save to the log'\n",
        "distill=-0.5 # help='distill loss weight')\n",
        "budget=0.2 #help='energy budget (relative)')\n",
        "exp_bdecay = 1 #action='store_true', help='exponential budget decay')\n",
        "mgpu =1 # action='store_true', help='enable using multiple gpus')\n",
        "skip1 = 0 #action='store_true', help='skip the first W update')\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2z8JzOIFY8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up random seeds\n",
        "torch.manual_seed(117)\n",
        "if cuda:\n",
        "  torch.cuda.manual_seed(117)\n",
        "  np.random.seed(117)\n",
        "  random.seed(117)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrclXY9xK541",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get training and validation data loaders\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.Resize(32),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./sample_data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "tr_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=num_workers)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./sample_data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(testset, batch_size=val_batch_size,\n",
        "                                         shuffle=False, num_workers=num_workers)\n",
        "\n",
        "train_loader4eval = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFIfOuPxMm5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get network model\n",
        "model, teacher_model = get_net_model(net=net, pretrained_dataset=dataset, dropout=(not nodp),\n",
        "                                         pretrained=not randinit, input_mask=input_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5JKdWQ9NwMR",
        "colab_type": "code",
        "outputId": "1c05cf44-7400-4db8-d232-f7246cc5cd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyLeNet5(\n",
              "  (features): Sequential(\n",
              "    (0): SparseConv2d(32, 32, 1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): SparseConv2d(14, 14, 6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAvhkr9iN2e5",
        "colab_type": "code",
        "outputId": "44dad21f-ffd5-4e43-b1fe-67b27a1c8a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "teacher_model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyLeNet5(\n",
              "  (features): Sequential(\n",
              "    (0): FixHWConv2d(32, 32, 1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): FixHWConv2d(14, 14, 6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIPPBdfPN63p",
        "colab_type": "code",
        "outputId": "6ad47277-5dbf-4076-eff7-e3742b8da8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        " # for energy estimate\n",
        "print('================model energy summary================')\n",
        "energy_info = build_energy_info(model)\n",
        "energy_estimator = lambda m: sum(energy_eval2(m, energy_info, verbose=False).values())\n",
        "energy_estimator_relaxed = lambda m: sum(energy_eval2_relax(m, energy_info, verbose=False).values())\n",
        "\n",
        "reset_Xenergy_cache(energy_info)\n",
        "cur_energy = sum(energy_eval2(model, energy_info, verbose=True).values())\n",
        "cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "\n",
        "dense_model = fill_model_weights(copy.deepcopy(model), 1.0)\n",
        "budget_ub = energy_estimator_relaxed(dense_model)\n",
        "zero_model = fill_model_weights(copy.deepcopy(model), 0.0)\n",
        "budget_lb = energy_estimator_relaxed(zero_model)\n",
        "\n",
        "del zero_model, dense_model\n",
        "budget = max(budget, budget_lb / budget_ub)\n",
        "\n",
        "proj_func = lambda m, budget, grad=False, in_place=True: energy_proj2(m, energy_info, budget, grad=grad,\n",
        "                                                                         in_place=in_place, param_name='weight')\n",
        "print('energy on dense DNN:{:.4e}, on zero DNN:{:.4e}, normalized_lb={:.4e}'.format(budget_ub, budget_lb,\n",
        "                                                                                        budget_lb / budget_ub))\n",
        "print('energy on current DNN:{:.4e}, normalized={:.4e}'.format(cur_energy, cur_energy / budget_ub))\n",
        "print('====================================================')\n",
        "print('current energy {:.4e}, relaxed: {:.4e}'.format(cur_energy, cur_energy_relaxed))\n",
        "netl2wd = l2wd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================model energy summary================\n",
            "Layer: features.0, input shape: (1, 32, 32), output shape: (6, 28, 28), weight shape: torch.Size([6, 1, 5, 5])\n",
            "Layer: features.0, W_energy=2.07e+05, C_energy=3.53e+05, X_energy=1.38e+06\n",
            "Layer: features.3, input shape: (6, 14, 14), output shape: (16, 10, 10), weight shape: torch.Size([16, 6, 5, 5])\n",
            "Layer: features.3, W_energy=8.50e+05, C_energy=7.20e+05, X_energy=9.75e+05\n",
            "Layer: classifier.0, W_energy=9.94e+06, C_energy=1.44e+05, X_energy=1.74e+05\n",
            "Layer: classifier.2, W_energy=2.09e+06, C_energy=3.02e+04, X_energy=5.52e+04\n",
            "Layer: classifier.4, W_energy=1.74e+05, C_energy=2.52e+03, X_energy=2.01e+04\n",
            "energy on dense DNN:1.7108e+07, on zero DNN:2.6049e+06, normalized_lb=1.5227e-01\n",
            "energy on current DNN:1.7108e+07, normalized=1.0000e+00\n",
            "====================================================\n",
            "current energy 1.7108e+07, relaxed: 1.7108e+07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DPnKghfOsXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if cuda:\n",
        "  if distill > 0.0:\n",
        "    teacher_model.cuda()\n",
        "model.cuda()\n",
        "loss_func = lambda m, x, y: joint_loss(model=m, data=x, target=y, teacher_model=teacher_model, distill=distill)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AjAdtyVPLUL",
        "colab_type": "code",
        "outputId": "24898ff0-ce98-4194-cf74-5b87a160d547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if eval or dataset != 'imagenet':\n",
        "  val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "  print('**Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(val_loss, val_acc1,\n",
        "                                                                                              val_acc5))\n",
        "# also evaluate training data\n",
        "  tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "  print('###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1, tr_acc5))\n",
        "else:\n",
        "    val_acc1 = 0.0\n",
        "    print('For imagenet, skip the first validation evaluation.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**Validation loss:2.3018e+00, top-1 accuracy:0.05770, top-5 accuracy:0.49600\n",
            "###Training loss:2.3023e+00, top-1 accuracy:0.06018, top-5 accuracy:0.49382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5YYKC3iPltD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_file = None\n",
        "\n",
        "energy_step = math.ceil(max(0.0, cur_energy - budget * budget_ub) / ((len(tr_loader) * epochs) / proj_int))\n",
        "\n",
        "energy_decay_factor = min(1.0, (budget * budget_ub) / cur_energy) ** \\\n",
        "                          (1.0 / ((len(tr_loader) * epochs) / proj_int))\n",
        "\n",
        "optimizer = torch.optim.SGD(filtered_parameters(model, param_name='input_mask', inverse=True), lr=0.1, momentum=0.9, weight_decay=netl2wd)\n",
        "if input_mask:\n",
        "  Xoptimizer = torch.optim.Adam(filtered_parameters(model, param_name='input_mask', inverse=False), lr=0.01, weight_decay=xl2wd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQQYzP1ZQSTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cur_budget = cur_energy_relaxed\n",
        "lr = lr_m\n",
        "xlr = xlr\n",
        "cur_sparsity = model_sparsity(model)\n",
        "\n",
        "best_acc_pruned = None\n",
        "Xbudget = 0.9\n",
        "iter_idx = 0\n",
        "\n",
        "W_proj_time = 0.0\n",
        "W_proj_time_cnt = 1e-15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40zehJ4aRi9K",
        "colab_type": "code",
        "outputId": "aa759bd9-730d-4ea3-e5b9-f457c7d03cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while True:\n",
        "        # update W\n",
        "        if not (skip1 and iter_idx == 0):\n",
        "            t_begin = time.time()\n",
        "            log_tic = t_begin\n",
        "            for epoch in range(epochs):\n",
        "                for batch_idx, (data, target) in enumerate(tr_loader):\n",
        "                    model.train()\n",
        "                    if cuda:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    loss = loss_func(model, data, target)\n",
        "                    # update network weights\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if proj_int == 1 or (batch_idx > 0 and batch_idx % proj_int == 0) or batch_idx == len(tr_loader) - 1:\n",
        "                        temp_tic = time.time()\n",
        "                        proj_func(model, cur_budget)\n",
        "                        W_proj_time += time.time() - temp_tic\n",
        "                        W_proj_time_cnt += 1\n",
        "                        #if epoch == epochs - 1 and batch_idx >= len(tr_loader) - 1 - proj_int:\n",
        "                        #    cur_budget = budget * budget_ub\n",
        "                        #else:\n",
        "                        #    if exp_bdecay:\n",
        "                        #        cur_budget = max(cur_budget * energy_decay_factor, budget * budget_ub)\n",
        "                        #    else:\n",
        "                        #        cur_budget = max(cur_budget - energy_step, budget * budget_ub)\n",
        "                        if exp_bdecay:\n",
        "                          cur_budget = max(cur_budget * energy_decay_factor, budget * budget_ub)\n",
        "                        else:\n",
        "                          cur_budget = max(cur_budget - energy_step, budget * budget_ub)\n",
        "                    #print(batch_idx)\n",
        "\n",
        "                    if batch_idx % log_interval == 0:\n",
        "                        print('======================================================')\n",
        "                        print('+-------------- epoch {}, batch {}/{} ----------------+'.format(epoch, batch_idx,\n",
        "                                                                                               len(tr_loader)))\n",
        "                        log_toc = time.time()\n",
        "                        print(\n",
        "                            'primal update: net loss={:.4e}, lr={:.4e}, current normalized budget: {:.4e}, time_elapsed={:.3f}s, averaged projection_time {}'.format(\n",
        "                                loss.item(), optimizer.param_groups[0]['lr'], cur_budget / budget_ub, log_toc - log_tic, W_proj_time / W_proj_time_cnt))\n",
        "                        log_tic = time.time()\n",
        "                        if batch_idx % proj_int == 0:\n",
        "                            cur_sparsity = model_sparsity(model)\n",
        "                        print('sparsity:{}'.format(cur_sparsity))\n",
        "                        print(layers_stat(model, param_names='weight', param_filter=lambda p: p.dim() > 1))\n",
        "                        print('+-----------------------------------------------------+')\n",
        "\n",
        "                cur_energy = energy_estimator(model)\n",
        "                cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "                cur_sparsity = model_sparsity(model)\n",
        "                if epoch % test_interval == 0:\n",
        "                    val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "\n",
        "                    # also evaluate training data\n",
        "                    tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "                    print('###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1,\n",
        "                                                                                                         tr_acc5))\n",
        "\n",
        "                    print(\n",
        "                        '***Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}, current normalized energy:{:.4e}, {:.4e}(relaxed), sparsity: {:.4e}'.format(\n",
        "                            val_loss, val_acc1,\n",
        "                            val_acc5, cur_energy / budget_ub, cur_energy_relaxed / budget_ub, cur_sparsity))\n",
        "                    # save current model\n",
        "                    model_snapshot(model, os.path.join(logdir, 'primal_model_latest.pkl'))\n",
        "\n",
        "                if save_interval > 0 and epoch % save_interval == 0:\n",
        "                    model_snapshot(model, os.path.join(logdir, 'Wprimal_model_epoch{}_{}.pkl'.format(iter_idx, epoch)))\n",
        "\n",
        "                elapse_time = time.time() - t_begin\n",
        "                speed_epoch = elapse_time / (1 + epoch)\n",
        "                eta = speed_epoch * (epochs - epoch)\n",
        "                print(\"Updating Weights, Elapsed {:.2f}s, ets {:.2f}s\".format(elapse_time, eta))\n",
        "\n",
        "        if not input_mask:\n",
        "            print(\"Complete weights training.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Continue to train input mask.\")\n",
        "\n",
        "        #if best_acc_pruned is not None and val_acc1 <= best_acc_pruned:\n",
        "        #    print(\"Pruned accuracy does not improve, stop here!\")\n",
        "        #    break\n",
        "        #best_acc_pruned = val_acc1\n",
        "\n",
        "        # update X\n",
        "        t_begin = time.time()\n",
        "        log_tic = t_begin\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, (data, target) in enumerate(tr_loader):\n",
        "                model.train()\n",
        "                Xoptimizer.param_groups[0]['lr'] = xlr\n",
        "                if cuda:\n",
        "                    data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                loss = loss_func(model, data, target)\n",
        "                # update network weights\n",
        "                Xoptimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                Xoptimizer.step()\n",
        "                clamp_model_weights(model, min=0.0, max=1.0, param_name='input_mask')\n",
        "\n",
        "                if (batch_idx > 0 and batch_idx % proj_int == 0) or batch_idx == len(tr_loader) - 1:\n",
        "                    l0proj(model, Xbudget, param_name='input_mask')\n",
        "\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    print('======================================================')\n",
        "                    print('+-------------- epoch {}, batch {}/{} ----------------+'.format(epoch, batch_idx,\n",
        "                                                                                           len(tr_loader)))\n",
        "                    log_toc = time.time()\n",
        "                    print('primal update: net loss={:.4e}, xlr={:.4e}, time_elapsed={:.3f}s'.format(\n",
        "                            loss.item(), Xoptimizer.param_groups[0]['lr'], log_toc - log_tic))\n",
        "                    log_tic = time.time()\n",
        "                    if batch_idx % proj_int == 0:\n",
        "                        cur_sparsity = model_sparsity(model, param_name='input_mask')\n",
        "                    print('sparsity:{}'.format(cur_sparsity))\n",
        "                    print(layers_stat(model, param_names='input_mask'))\n",
        "                    print('+-----------------------------------------------------+')\n",
        "\n",
        "            cur_energy = energy_estimator(model)\n",
        "            cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "            cur_sparsity = model_sparsity(model, param_name='input_mask')\n",
        "            if epoch % test_interval == 0:\n",
        "\n",
        "                val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "\n",
        "                # also evaluate training data\n",
        "                tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "                print(\n",
        "                    '###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1,\n",
        "                                                                                                   tr_acc5))\n",
        "\n",
        "                print(\n",
        "                    '***Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}, current normalized energy:{:.4e}, {:.4e}(relaxed), sparsity: {:.4e}'.format(\n",
        "                        val_loss, val_acc1,\n",
        "                        val_acc5, cur_energy / budget_ub, cur_energy_relaxed / budget_ub, cur_sparsity))\n",
        "                # save current model\n",
        "                model_snapshot(model, os.path.join(logdir, 'primal_model_latest.pkl'))\n",
        "\n",
        "            if save_interval > 0 and epoch % save_interval == 0:\n",
        "                model_snapshot(model, os.path.join(logdir, 'Xprimal_model_epoch{}_{}.pkl'.format(iter_idx, epoch)))\n",
        "\n",
        "            elapse_time = time.time() - t_begin\n",
        "            speed_epoch = elapse_time / (1 + epoch)\n",
        "            eta = speed_epoch * (epochs - epoch)\n",
        "            print(\"Updating input mask, Elapsed {:.2f}s, ets {:.2f}s\".format(elapse_time, eta))\n",
        "\n",
        "        round_model_weights(model, param_name='input_mask')\n",
        "        # refresh X_energy_cache\n",
        "        reset_Xenergy_cache(energy_info)\n",
        "        cur_energy = energy_estimator(model)\n",
        "        cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "        if best_acc_pruned is not None and val_acc1 <= best_acc_pruned:\n",
        "            print(\"Pruned accuracy does not improve, stop here!\")\n",
        "            break\n",
        "        best_acc_pruned = val_acc1\n",
        "\n",
        "        iter_idx += 1\n",
        "        Xbudget -= 0.1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=1.7716e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=0.338s, averaged projection_time 0.003079493840535482\n",
            "sparsity:0.29850333496014314\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3101e-01, max=2.0448e+00, nnz=0.6667\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7324e-02, max=1.5201e+00, nnz=0.3017\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5974e-02, max=1.5890e+00, nnz=0.2752\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8796e-02, max=1.8904e+00, nnz=0.3753\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5790e-02, max=1.9455e+00, nnz=0.6333\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=9.6261e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.219s, averaged projection_time 0.0030948821812459867\n",
            "sparsity:0.052741174556694324\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3259e-01, max=2.1176e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.6259e-02, max=1.4689e+00, nnz=0.0842\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5931e-02, max=1.6837e+00, nnz=0.0408\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8586e-02, max=1.8716e+00, nnz=0.0873\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6223e-02, max=1.9262e+00, nnz=0.2250\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=5.5922e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.187s, averaged projection_time 0.0030953149921846704\n",
            "sparsity:0.05287131934276883\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3192e-01, max=2.2696e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.6200e-02, max=1.5176e+00, nnz=0.0825\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5966e-02, max=1.6821e+00, nnz=0.0408\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8559e-02, max=1.8529e+00, nnz=0.0879\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5781e-02, max=1.9070e+00, nnz=0.2310\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=7.0770e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.177s, averaged projection_time 0.003073976590083196\n",
            "sparsity:0.05280624694973158\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3292e-01, max=2.0358e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.6514e-02, max=1.4795e+00, nnz=0.0833\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5945e-02, max=1.7158e+00, nnz=0.0409\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8292e-02, max=1.8345e+00, nnz=0.0873\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4078e-02, max=1.8880e+00, nnz=0.2286\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=3.5735e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.165s, averaged projection_time 0.003052414574238084\n",
            "sparsity:0.05277371075321295\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3342e-01, max=2.3968e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.6608e-02, max=1.4561e+00, nnz=0.0838\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5870e-02, max=1.7270e+00, nnz=0.0405\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8225e-02, max=1.8162e+00, nnz=0.0882\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4335e-02, max=1.8692e+00, nnz=0.2321\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=6.3791e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.193s, averaged projection_time 0.003080235188265881\n",
            "sparsity:0.05280624694973158\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3100e-01, max=2.1602e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.6358e-02, max=1.4525e+00, nnz=0.0833\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5812e-02, max=1.7267e+00, nnz=0.0407\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8101e-02, max=1.7981e+00, nnz=0.0880\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.3750e-02, max=1.8506e+00, nnz=0.2286\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=1.2381e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.178s, averaged projection_time 0.003082258659496642\n",
            "sparsity:0.05293639173580608\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2937e-01, max=2.0468e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.6325e-02, max=1.4843e+00, nnz=0.0817\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5790e-02, max=1.7644e+00, nnz=0.0407\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.7872e-02, max=1.7802e+00, nnz=0.0886\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4450e-02, max=1.8321e+00, nnz=0.2357\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=7.9753e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.220s, averaged projection_time 0.0030715831301429057\n",
            "sparsity:0.05311534081665853\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2928e-01, max=2.2408e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.5932e-02, max=1.4947e+00, nnz=0.0792\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5884e-02, max=1.7981e+00, nnz=0.0411\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.7981e-02, max=1.7625e+00, nnz=0.0885\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4782e-02, max=1.8139e+00, nnz=0.2321\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=4.0109e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.174s, averaged projection_time 0.0030508818547370026\n",
            "sparsity:0.05321294940621441\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3146e-01, max=2.2259e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.5283e-02, max=1.5143e+00, nnz=0.0783\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5739e-02, max=1.7589e+00, nnz=0.0410\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.7860e-02, max=1.7449e+00, nnz=0.0898\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4395e-02, max=1.7958e+00, nnz=0.2369\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=3.7104e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.177s, averaged projection_time 0.003044436695755169\n",
            "sparsity:0.053017732227102654\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3238e-01, max=2.1367e+00, nnz=0.0867\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.5815e-02, max=1.4494e+00, nnz=0.0804\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5711e-02, max=1.7867e+00, nnz=0.0407\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.7683e-02, max=1.7275e+00, nnz=0.0898\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.2270e-02, max=1.7779e+00, nnz=0.2310\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:9.9907e-02, top-1 accuracy:0.97268, top-5 accuracy:0.99790\n",
            "***Validation loss:1.0291e-01, top-1 accuracy:0.97240, top-5 accuracy:0.99770, current normalized energy:1.9998e-01, 1.9998e-01(relaxed), sparsity: 5.3083e-02\n",
            "Updating Weights, Elapsed 23.95s, ets 23.95s\n",
            "Continue to train input mask.\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=2.3325e-01, xlr=1.0000e-04, time_elapsed=0.325s\n",
            "sparsity:1.0\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=9.9983e-01, mean=9.9998e-01, max=1.0000e+00, nnz=1.0000\n",
            "          features.3abs(W): min=9.9983e-01, mean=9.9996e-01, max=1.0000e+00, nnz=1.0000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=1.1393e-01, xlr=1.0000e-04, time_elapsed=1.234s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4500e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7262e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=1.0826e-01, xlr=1.0000e-04, time_elapsed=1.157s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4451e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7229e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=7.5646e-02, xlr=1.0000e-04, time_elapsed=1.202s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4402e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7194e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=1.6598e-01, xlr=1.0000e-04, time_elapsed=1.178s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4365e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7168e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=4.2294e-01, xlr=1.0000e-04, time_elapsed=1.205s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4320e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7137e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=9.4932e-03, xlr=1.0000e-04, time_elapsed=1.184s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4281e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7107e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=4.5243e-02, xlr=1.0000e-04, time_elapsed=1.207s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4246e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7081e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=5.9763e-02, xlr=1.0000e-04, time_elapsed=1.186s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4210e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7055e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=6.5474e-03, xlr=1.0000e-04, time_elapsed=1.190s\n",
            "sparsity:0.7\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=8.4168e-01, max=1.0000e+00, nnz=0.8457\n",
            "          features.3abs(W): min=0.0000e+00, mean=5.7026e-01, max=1.0000e+00, nnz=0.5731\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:1.0544e-01, top-1 accuracy:0.97120, top-5 accuracy:0.99772\n",
            "***Validation loss:1.0800e-01, top-1 accuracy:0.97060, top-5 accuracy:0.99670, current normalized energy:1.8175e-01, 1.8221e-01(relaxed), sparsity: 7.0000e-01\n",
            "Updating input mask, Elapsed 24.20s, ets 24.20s\n",
            "Pruned accuracy does not improve, stop here!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUJ6ERWIZTiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filtered_parameters_name(model, param_name, inverse=False):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(name)\n",
        "        if inverse != (name.endswith(param_name)):\n",
        "            yield param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDM2FM2KZ41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = filtered_parameters(model, param_name='input_mask', inverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luo5GnXmKeEl",
        "colab_type": "code",
        "outputId": "614f4561-3397-4b78-e719-e691225e0a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while True:\n",
        "        # update W\n",
        "        if not (skip1 and iter_idx == 0):\n",
        "            t_begin = time.time()\n",
        "            log_tic = t_begin\n",
        "            for epoch in range(epochs):\n",
        "                for batch_idx, (data, target) in enumerate(tr_loader):\n",
        "                    model.train()\n",
        "                    if cuda:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    loss = loss_func(model, data, target)\n",
        "                    # update network weights\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if proj_int == 1 or (batch_idx > 0 and batch_idx % proj_int == 0) or batch_idx == len(tr_loader) - 1:\n",
        "                        temp_tic = time.time()\n",
        "                        proj_func(model, cur_budget)\n",
        "                        W_proj_time += time.time() - temp_tic\n",
        "                        W_proj_time_cnt += 1\n",
        "                        if epoch == epochs - 1 and batch_idx >= len(tr_loader) - 1 - proj_int:\n",
        "                            cur_budget = budget * budget_ub\n",
        "                        else:\n",
        "                            if exp_bdecay:\n",
        "                                cur_budget = max(cur_budget * energy_decay_factor, budget * budget_ub)\n",
        "                            else:\n",
        "                                cur_budget = max(cur_budget - energy_step, budget * budget_ub)\n",
        "                    #print(batch_idx)\n",
        "\n",
        "                    if batch_idx % log_interval == 0:\n",
        "                        print('======================================================')\n",
        "                        print('+-------------- epoch {}, batch {}/{} ----------------+'.format(epoch, batch_idx,\n",
        "                                                                                               len(tr_loader)))\n",
        "                        log_toc = time.time()\n",
        "                        print(\n",
        "                            'primal update: net loss={:.4e}, lr={:.4e}, current normalized budget: {:.4e}, time_elapsed={:.3f}s, averaged projection_time {}'.format(\n",
        "                                loss.item(), optimizer.param_groups[0]['lr'], cur_budget / budget_ub, log_toc - log_tic, W_proj_time / W_proj_time_cnt))\n",
        "                        log_tic = time.time()\n",
        "                        if batch_idx % proj_int == 0:\n",
        "                            cur_sparsity = model_sparsity(model)\n",
        "                        print('sparsity:{}'.format(cur_sparsity))\n",
        "                        print(layers_stat(model, param_names='weight', param_filter=lambda p: p.dim() > 1))\n",
        "                        print('+-----------------------------------------------------+')\n",
        "\n",
        "                cur_energy = energy_estimator(model)\n",
        "                cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "                cur_sparsity = model_sparsity(model)\n",
        "                if epoch % test_interval == 0:\n",
        "                    val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "\n",
        "                    # also evaluate training data\n",
        "                    tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "                    print('###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1,\n",
        "                                                                                                         tr_acc5))\n",
        "\n",
        "                    print(\n",
        "                        '***Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}, current normalized energy:{:.4e}, {:.4e}(relaxed), sparsity: {:.4e}'.format(\n",
        "                            val_loss, val_acc1,\n",
        "                            val_acc5, cur_energy / budget_ub, cur_energy_relaxed / budget_ub, cur_sparsity))\n",
        "                    # save current model\n",
        "                    model_snapshot(model, os.path.join(logdir, 'primal_model_latest.pkl'))\n",
        "\n",
        "                if save_interval > 0 and epoch % save_interval == 0:\n",
        "                    model_snapshot(model, os.path.join(logdir, 'Wprimal_model_epoch{}_{}.pkl'.format(iter_idx, epoch)))\n",
        "\n",
        "                elapse_time = time.time() - t_begin\n",
        "                speed_epoch = elapse_time / (1 + epoch)\n",
        "                eta = speed_epoch * (epochs - epoch)\n",
        "                print(\"Updating Weights, Elapsed {:.2f}s, ets {:.2f}s\".format(elapse_time, eta))\n",
        "\n",
        "        if not input_mask:\n",
        "            print(\"Complete weights training.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Continue to train input mask.\")\n",
        "\n",
        "        if best_acc_pruned is not None and val_acc1 <= best_acc_pruned:\n",
        "            print(\"Pruned accuracy does not improve, stop here!\")\n",
        "            break\n",
        "        best_acc_pruned = val_acc1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=2.3072e+00, lr=1.0000e-01, current normalized budget: 1.0000e+00, time_elapsed=0.366s, averaged projection_time 0.0\n",
            "sparsity:1.0\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=2.5658e-03, mean=9.8756e-02, max=1.9834e-01, nnz=1.0000\n",
            "          features.3abs(W): min=8.0428e-06, mean=4.0750e-02, max=8.1883e-02, nnz=1.0000\n",
            "        classifier.0abs(W): min=3.6417e-07, mean=2.5031e-02, max=5.0461e-02, nnz=1.0000\n",
            "        classifier.2abs(W): min=2.4086e-06, mean=4.6014e-02, max=9.1304e-02, nnz=1.0000\n",
            "        classifier.4abs(W): min=2.2155e-05, mean=5.2406e-02, max=1.0885e-01, nnz=1.0000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=7.5997e-01, lr=1.0000e-01, current normalized budget: 8.4233e-01, time_elapsed=1.334s, averaged projection_time 0.015898299217224122\n",
            "sparsity:0.827916056612982\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.7570e-01, max=7.6630e-01, nnz=0.8200\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.2418e-01, max=7.9206e-01, nnz=0.8762\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.4703e-02, max=7.0505e-01, nnz=0.8108\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=7.1412e-02, max=5.5932e-01, nnz=0.8902\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.5792e-01, max=1.4982e+00, nnz=0.9226\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=4.6049e-01, lr=1.0000e-01, current normalized budget: 7.0952e-01, time_elapsed=1.168s, averaged projection_time 0.010083496570587158\n",
            "sparsity:0.665072393037254\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.2003e-01, max=1.0082e+00, nnz=0.6533\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.3811e-01, max=9.0603e-01, nnz=0.7625\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.6597e-02, max=8.8788e-01, nnz=0.6324\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=7.6193e-02, max=6.1293e-01, nnz=0.7825\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.5446e-01, max=1.4795e+00, nnz=0.8488\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=2.9362e-01, lr=1.0000e-01, current normalized budget: 5.9765e-01, time_elapsed=1.222s, averaged projection_time 0.00793451468149821\n",
            "sparsity:0.5275256222547584\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.0963e-01, max=1.1523e+00, nnz=0.5267\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.3946e-01, max=8.8926e-01, nnz=0.6646\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.3725e-02, max=8.6508e-01, nnz=0.4819\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=7.6181e-02, max=6.6483e-01, nnz=0.6923\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.5134e-01, max=1.4652e+00, nnz=0.7643\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=2.1134e-01, lr=1.0000e-01, current normalized budget: 5.0342e-01, time_elapsed=1.188s, averaged projection_time 0.0068813741207122804\n",
            "sparsity:0.4063933626159102\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.5279e-01, max=1.1437e+00, nnz=0.5267\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.4492e-01, max=9.8522e-01, nnz=0.5929\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=3.9554e-02, max=9.4196e-01, nnz=0.3498\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=7.4105e-02, max=7.2743e-01, nnz=0.6033\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.4995e-01, max=1.4513e+00, nnz=0.7238\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=3.5446e-01, lr=1.0000e-01, current normalized budget: 4.2405e-01, time_elapsed=1.210s, averaged projection_time 0.0061080408096313476\n",
            "sparsity:0.3117943712380023\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.2487e-01, max=1.3843e+00, nnz=0.4133\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.3915e-01, max=1.1503e+00, nnz=0.4988\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=3.5404e-02, max=9.7243e-01, nnz=0.2549\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=6.9862e-02, max=6.7995e-01, nnz=0.5079\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.4615e-01, max=1.4354e+00, nnz=0.6583\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=5.6057e-02, lr=1.0000e-01, current normalized budget: 3.5719e-01, time_elapsed=1.177s, averaged projection_time 0.005784026781717936\n",
            "sparsity:0.2331706523507402\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.0493e-01, max=1.6906e+00, nnz=0.3067\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.3017e-01, max=1.2217e+00, nnz=0.4121\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=3.1120e-02, max=1.1403e+00, nnz=0.1846\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=6.3660e-02, max=7.3170e-01, nnz=0.3923\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.4151e-01, max=1.4212e+00, nnz=0.5762\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=1.3361e-01, lr=1.0000e-01, current normalized budget: 3.0087e-01, time_elapsed=1.179s, averaged projection_time 0.005408634458269392\n",
            "sparsity:0.1673011224987799\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.8941e-01, max=1.7647e+00, nnz=0.2467\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.1592e-01, max=1.3303e+00, nnz=0.3200\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=2.6443e-02, max=1.1171e+00, nnz=0.1305\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=5.3598e-02, max=7.2108e-01, nnz=0.2772\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.3286e-01, max=1.4063e+00, nnz=0.5024\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=9.5619e-02, lr=1.0000e-01, current normalized budget: 2.5343e-01, time_elapsed=1.187s, averaged projection_time 0.0051519334316253666\n",
            "sparsity:0.11361639824304538\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.6055e-01, max=1.6214e+00, nnz=0.1867\n",
            "          features.3abs(W): min=0.0000e+00, mean=9.6362e-02, max=1.4882e+00, nnz=0.2254\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=2.1551e-02, max=1.0653e+00, nnz=0.0872\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=4.3103e-02, max=7.1312e-01, nnz=0.1870\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.1936e-01, max=1.3923e+00, nnz=0.4071\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=2.3075e-01, lr=1.0000e-01, current normalized budget: 2.1347e-01, time_elapsed=1.173s, averaged projection_time 0.004947945806715224\n",
            "sparsity:0.06913941760208232\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3720e-01, max=1.5304e+00, nnz=0.1333\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.3235e-02, max=1.5167e+00, nnz=0.1379\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6167e-02, max=1.0526e+00, nnz=0.0517\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.1813e-02, max=7.0581e-01, nnz=0.1130\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.0841e-01, max=1.3784e+00, nnz=0.3321\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:1.7689e-01, top-1 accuracy:0.95120, top-5 accuracy:0.99602\n",
            "***Validation loss:1.6730e-01, top-1 accuracy:0.95610, top-5 accuracy:0.99620, current normalized energy:1.9998e-01, 1.9998e-01(relaxed), sparsity: 5.0675e-02\n",
            "Updating Weights, Elapsed 23.96s, ets 23.96s\n",
            "Continue to train input mask.\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=2.9340e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=0.289s, averaged projection_time 0.004815400914942964\n",
            "sparsity:0.9774361477143322\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=7.5492e-34, mean=1.2148e-01, max=1.4778e+00, nnz=1.0000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1917e-02, max=1.5270e+00, nnz=0.9992\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3603e-02, max=1.0418e+00, nnz=0.9723\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5322e-02, max=8.2705e-01, nnz=0.9946\n",
            "        classifier.4abs(W): min=2.0207e-27, mean=9.4026e-02, max=1.3732e+00, nnz=1.0000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=4.4476e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.253s, averaged projection_time 0.004601882054255559\n",
            "sparsity:0.050544981291687004\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2502e-01, max=1.8258e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1842e-02, max=1.7848e+00, nnz=0.1025\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3477e-02, max=1.1483e+00, nnz=0.0379\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5115e-02, max=8.1679e-01, nnz=0.0803\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=9.1205e-02, max=1.3595e+00, nnz=0.2571\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=3.9079e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.189s, averaged projection_time 0.004419272406059399\n",
            "sparsity:0.05074019847079876\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2158e-01, max=1.7153e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2157e-02, max=1.8352e+00, nnz=0.1017\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3470e-02, max=1.1975e+00, nnz=0.0378\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5192e-02, max=7.7493e-01, nnz=0.0815\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=9.1687e-02, max=1.3460e+00, nnz=0.2667\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=1.0431e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.207s, averaged projection_time 0.004321784742416874\n",
            "sparsity:0.05057751748820563\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2739e-01, max=2.0614e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2233e-02, max=1.8527e+00, nnz=0.1004\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3463e-02, max=1.2269e+00, nnz=0.0377\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5057e-02, max=8.6717e-01, nnz=0.0818\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.9688e-02, max=1.3326e+00, nnz=0.2595\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=1.1866e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.184s, averaged projection_time 0.004220144072575356\n",
            "sparsity:0.050642589881242886\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2982e-01, max=2.0189e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.3259e-02, max=1.8773e+00, nnz=0.0996\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3512e-02, max=1.1704e+00, nnz=0.0375\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5195e-02, max=8.8616e-01, nnz=0.0827\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=9.0521e-02, max=1.3193e+00, nnz=0.2655\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=2.1139e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.196s, averaged projection_time 0.00412573582596249\n",
            "sparsity:0.05075646656905808\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3252e-01, max=1.9891e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.3135e-02, max=1.8982e+00, nnz=0.0983\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3528e-02, max=1.1987e+00, nnz=0.0377\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5026e-02, max=9.1189e-01, nnz=0.0833\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7469e-02, max=1.3062e+00, nnz=0.2619\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=3.5818e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.169s, averaged projection_time 0.004080679509546849\n",
            "sparsity:0.050707662274280134\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3221e-01, max=1.9241e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2671e-02, max=1.8693e+00, nnz=0.0983\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3529e-02, max=1.2393e+00, nnz=0.0374\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5080e-02, max=9.9487e-01, nnz=0.0842\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7243e-02, max=1.2932e+00, nnz=0.2607\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=4.1287e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.192s, averaged projection_time 0.004000024097721751\n",
            "sparsity:0.050707662274280134\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3122e-01, max=1.8065e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1968e-02, max=1.9252e+00, nnz=0.0983\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3468e-02, max=1.2723e+00, nnz=0.0373\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5117e-02, max=8.7975e-01, nnz=0.0844\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7134e-02, max=1.2803e+00, nnz=0.2655\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=3.1449e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.216s, averaged projection_time 0.003924547940835185\n",
            "sparsity:0.05075646656905808\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2734e-01, max=1.8145e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1853e-02, max=1.9624e+00, nnz=0.0983\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3411e-02, max=1.2730e+00, nnz=0.0373\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4901e-02, max=9.2651e-01, nnz=0.0849\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6113e-02, max=1.2675e+00, nnz=0.2619\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=4.4318e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.170s, averaged projection_time 0.003956783076991205\n",
            "sparsity:0.050707662274280134\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3237e-01, max=1.7607e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1610e-02, max=2.0252e+00, nnz=0.0988\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3438e-02, max=1.2693e+00, nnz=0.0372\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4722e-02, max=9.2079e-01, nnz=0.0849\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5538e-02, max=1.2549e+00, nnz=0.2643\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:1.0522e-01, top-1 accuracy:0.97225, top-5 accuracy:0.99777\n",
            "***Validation loss:1.0970e-01, top-1 accuracy:0.97110, top-5 accuracy:0.99790, current normalized energy:1.9998e-01, 1.9998e-01(relaxed), sparsity: 5.0643e-02\n",
            "Updating Weights, Elapsed 24.04s, ets 24.04s\n",
            "Continue to train input mask.\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=8.4086e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=0.294s, averaged projection_time 0.003913345489096135\n",
            "sparsity:0.3128355295265983\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2972e-01, max=1.7400e+00, nnz=0.6667\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2158e-02, max=1.9960e+00, nnz=0.3171\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3498e-02, max=1.2840e+00, nnz=0.2724\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4735e-02, max=9.3556e-01, nnz=0.4599\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5410e-02, max=1.2501e+00, nnz=0.7833\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=3.5630e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.258s, averaged projection_time 0.0038840578059957487\n",
            "sparsity:0.050675126077761506\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2226e-01, max=1.7316e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1176e-02, max=1.9606e+00, nnz=0.0992\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3351e-02, max=1.2976e+00, nnz=0.0371\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4365e-02, max=8.8133e-01, nnz=0.0845\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5773e-02, max=1.2377e+00, nnz=0.2726\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=1.6374e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.178s, averaged projection_time 0.003826746573814979\n",
            "sparsity:0.05082153896209533\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2655e-01, max=1.7864e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.0909e-02, max=2.0333e+00, nnz=0.0975\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3379e-02, max=1.3683e+00, nnz=0.0370\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4422e-02, max=9.0879e-01, nnz=0.0858\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6949e-02, max=1.2254e+00, nnz=0.2774\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=2.2266e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.164s, averaged projection_time 0.003759792091649607\n",
            "sparsity:0.05091914755165121\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2099e-01, max=1.8341e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1183e-02, max=2.0920e+00, nnz=0.0992\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3363e-02, max=1.3722e+00, nnz=0.0370\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4629e-02, max=8.0755e-01, nnz=0.0860\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7248e-02, max=1.2132e+00, nnz=0.2810\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=9.7247e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.174s, averaged projection_time 0.0037656039522405258\n",
            "sparsity:0.05091914755165121\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2427e-01, max=1.8074e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1539e-02, max=2.0383e+00, nnz=0.0996\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3332e-02, max=1.3737e+00, nnz=0.0367\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4691e-02, max=9.3514e-01, nnz=0.0870\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.8554e-02, max=1.2011e+00, nnz=0.2821\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=1.0470e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.227s, averaged projection_time 0.0037505336168433437\n",
            "sparsity:0.05088661135513259\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2329e-01, max=1.6453e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2148e-02, max=2.0596e+00, nnz=0.1000\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3418e-02, max=1.4258e+00, nnz=0.0367\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4781e-02, max=9.4802e-01, nnz=0.0873\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6602e-02, max=1.1891e+00, nnz=0.2738\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=9.4691e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.195s, averaged projection_time 0.003732596674273091\n",
            "sparsity:0.05104929233772572\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2522e-01, max=1.7695e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1256e-02, max=2.0411e+00, nnz=0.0979\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3458e-02, max=1.4507e+00, nnz=0.0371\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4753e-02, max=1.0108e+00, nnz=0.0873\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7223e-02, max=1.1773e+00, nnz=0.2726\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=3.3526e-03, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.155s, averaged projection_time 0.0037299024966336034\n",
            "sparsity:0.051228241418578166\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2976e-01, max=1.8598e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.0848e-02, max=1.9807e+00, nnz=0.0958\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3516e-02, max=1.4442e+00, nnz=0.0370\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5042e-02, max=9.3246e-01, nnz=0.0883\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=9.0183e-02, max=1.1655e+00, nnz=0.2833\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=5.0368e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.198s, averaged projection_time 0.003688584512739039\n",
            "sparsity:0.05116316902554091\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2938e-01, max=1.8571e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1311e-02, max=2.0452e+00, nnz=0.0967\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3678e-02, max=1.5520e+00, nnz=0.0371\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.4972e-02, max=9.7352e-01, nnz=0.0872\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7503e-02, max=1.1539e+00, nnz=0.2810\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=1.3686e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.163s, averaged projection_time 0.0036743810708574254\n",
            "sparsity:0.05127704571335611\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2885e-01, max=1.7560e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.0973e-02, max=2.1032e+00, nnz=0.0950\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3741e-02, max=1.5109e+00, nnz=0.0370\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5131e-02, max=8.9359e-01, nnz=0.0884\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7353e-02, max=1.1424e+00, nnz=0.2869\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:9.6507e-02, top-1 accuracy:0.97382, top-5 accuracy:0.99775\n",
            "***Validation loss:9.8224e-02, top-1 accuracy:0.97370, top-5 accuracy:0.99740, current normalized energy:1.9996e-01, 1.9996e-01(relaxed), sparsity: 5.1310e-02\n",
            "Updating Weights, Elapsed 23.69s, ets 23.69s\n",
            "Continue to train input mask.\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=9.3212e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=0.330s, averaged projection_time 0.0036486015252187743\n",
            "sparsity:0.30740198470798763\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3405e-01, max=1.7127e+00, nnz=0.6667\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1288e-02, max=2.0801e+00, nnz=0.3171\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3802e-02, max=1.4985e+00, nnz=0.2687\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5368e-02, max=7.9515e-01, nnz=0.4453\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.8350e-02, max=1.1381e+00, nnz=0.7738\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=1.9489e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.225s, averaged projection_time 0.003649533611454376\n",
            "sparsity:0.05140719049943061\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3404e-01, max=1.7801e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1023e-02, max=2.1597e+00, nnz=0.0938\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3835e-02, max=1.5315e+00, nnz=0.0371\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5406e-02, max=8.7707e-01, nnz=0.0897\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6516e-02, max=1.1268e+00, nnz=0.2810\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=1.0652e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.189s, averaged projection_time 0.0036329852034714047\n",
            "sparsity:0.05137465430291199\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3739e-01, max=1.8017e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2073e-02, max=2.2405e+00, nnz=0.0942\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3848e-02, max=1.5407e+00, nnz=0.0369\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5360e-02, max=9.2281e-01, nnz=0.0907\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4208e-02, max=1.1155e+00, nnz=0.2786\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=1.8219e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.204s, averaged projection_time 0.0036272085629976713\n",
            "sparsity:0.05168374816983894\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3822e-01, max=1.9065e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1646e-02, max=2.2082e+00, nnz=0.0904\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3971e-02, max=1.5121e+00, nnz=0.0371\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5454e-02, max=8.7322e-01, nnz=0.0909\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7129e-02, max=1.1044e+00, nnz=0.2929\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=3.3728e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.233s, averaged projection_time 0.0035988240508559328\n",
            "sparsity:0.051634943875061005\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3997e-01, max=2.0002e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1671e-02, max=2.1384e+00, nnz=0.0908\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.3983e-02, max=1.5460e+00, nnz=0.0370\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5333e-02, max=8.6421e-01, nnz=0.0911\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7302e-02, max=1.0934e+00, nnz=0.2917\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=2.5261e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.185s, averaged projection_time 0.0035945378154157155\n",
            "sparsity:0.05171628436635757\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3391e-01, max=1.9573e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1196e-02, max=2.1510e+00, nnz=0.0900\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.4031e-02, max=1.5467e+00, nnz=0.0372\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5586e-02, max=8.4176e-01, nnz=0.0913\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.9387e-02, max=1.0825e+00, nnz=0.2881\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=2.3076e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.212s, averaged projection_time 0.0035832067679243476\n",
            "sparsity:0.05171628436635757\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3795e-01, max=1.9589e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.0986e-02, max=2.1910e+00, nnz=0.0900\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.4038e-02, max=1.5240e+00, nnz=0.0372\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5698e-02, max=8.9195e-01, nnz=0.0914\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7587e-02, max=1.0718e+00, nnz=0.2893\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=5.3348e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.197s, averaged projection_time 0.003548120233145627\n",
            "sparsity:0.05171628436635757\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3520e-01, max=1.8076e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1034e-02, max=2.2929e+00, nnz=0.0900\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.4017e-02, max=1.5141e+00, nnz=0.0369\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5587e-02, max=8.4331e-01, nnz=0.0920\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6905e-02, max=1.0611e+00, nnz=0.2952\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=2.3719e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.207s, averaged projection_time 0.0035508904009234183\n",
            "sparsity:0.05171628436635757\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3651e-01, max=2.0197e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.1357e-02, max=2.3526e+00, nnz=0.0900\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.4011e-02, max=1.5138e+00, nnz=0.0371\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5668e-02, max=8.1428e-01, nnz=0.0926\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5350e-02, max=1.0505e+00, nnz=0.2810\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=4.3605e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.208s, averaged projection_time 0.0035321417675223403\n",
            "sparsity:0.051634943875061005\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.4388e-01, max=2.0796e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.2011e-02, max=2.3320e+00, nnz=0.0908\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.4068e-02, max=1.5216e+00, nnz=0.0369\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.5650e-02, max=8.5749e-01, nnz=0.0921\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4368e-02, max=1.0401e+00, nnz=0.2857\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:1.0684e-01, top-1 accuracy:0.97215, top-5 accuracy:0.99795\n",
            "***Validation loss:1.1351e-01, top-1 accuracy:0.96990, top-5 accuracy:0.99740, current normalized energy:1.9998e-01, 1.9998e-01(relaxed), sparsity: 5.1602e-02\n",
            "Updating Weights, Elapsed 24.00s, ets 24.00s\n",
            "Continue to train input mask.\n",
            "Pruned accuracy does not improve, stop here!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3jfaHrKk_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}