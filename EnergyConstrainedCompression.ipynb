{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnergyConstrainedCompression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panda1230/pytorch-cnn-visualizations/blob/master/EnergyConstrainedCompression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiaeQP2rEpP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "from models import get_net_model\n",
        "from proj_utils import fill_model_weights, layers_stat, model_sparsity, filtered_parameters, \\\n",
        "    l0proj, round_model_weights, clamp_model_weights\n",
        "from sa_energy_model import build_energy_info, energy_eval2, energy_eval2_relax, energy_proj2, \\\n",
        "    reset_Xenergy_cache\n",
        "from utils import get_data_loaders, joint_loss, eval_loss_acc1_acc5, model_snapshot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl4qPE6lFFat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net='lenet-5'\n",
        "dataset = 'mnist-32'\n",
        "batch_size= 64\n",
        "val_batch_size=64\n",
        "num_workers =8 \n",
        "epochs =1\n",
        "lr_m = 0.001, #'learning rate'\n",
        "xlr =1e-4 #'learning rate for input mask'\n",
        "l2wd =1e-4 #'l2 weight decay'\n",
        "xl2wd=1e-5 #l2 weight decay (for input mask)'\n",
        "momentum =0.9 #momentum\n",
        "proj_int=10 #'how many batches for each projection'\n",
        "nodp = 1 #action='store_true', help='turn off dropout'\n",
        "input_mask=1 # action='store_true', help='enable input mask'\n",
        "randinit =1 #action='store_true', help='use random init'\n",
        "eval =1 #action='store_true', help='evaluate testset in the begining')\n",
        "#parser.add_argument('--pretrain', default=None, help='file to load pretrained model')\n",
        "#parser.add_argument('--eval', action='store_true', help='evaluate testset in the begining')\n",
        "#parser.add_argument('--seed', type=int, default=117, help='random seed')\n",
        "log_interval =100 # help='how many batches to wait before logging training status')\n",
        "test_interval = 1 # help='how many epochs to wait before another test')\n",
        "save_interval=10# help='how many epochs to wait before save a model')\n",
        "logdir = './sample_data/' #help='folder to save to the log'\n",
        "distill=-0.5 # help='distill loss weight')\n",
        "budget=0.2 #help='energy budget (relative)')\n",
        "exp_bdecay = 1 #action='store_true', help='exponential budget decay')\n",
        "mgpu =1 # action='store_true', help='enable using multiple gpus')\n",
        "skip1 = 0 #action='store_true', help='skip the first W update')\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2z8JzOIFY8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up random seeds\n",
        "torch.manual_seed(117)\n",
        "if cuda:\n",
        "  torch.cuda.manual_seed(117)\n",
        "  np.random.seed(117)\n",
        "  random.seed(117)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrclXY9xK541",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get training and validation data loaders\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.Resize(32),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./sample_data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "tr_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=num_workers)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./sample_data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(testset, batch_size=val_batch_size,\n",
        "                                         shuffle=False, num_workers=num_workers)\n",
        "\n",
        "train_loader4eval = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFIfOuPxMm5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get network model\n",
        "model, teacher_model = get_net_model(net=net, pretrained_dataset=dataset, dropout=(not nodp),\n",
        "                                         pretrained=not randinit, input_mask=input_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5JKdWQ9NwMR",
        "colab_type": "code",
        "outputId": "7fd9730f-df42-448b-f749-605b759317dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyLeNet5(\n",
              "  (features): Sequential(\n",
              "    (0): SparseConv2d(32, 32, 1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): SparseConv2d(14, 14, 6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAvhkr9iN2e5",
        "colab_type": "code",
        "outputId": "a26aa77e-31d3-4bb2-ab90-22480ddc482b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "teacher_model"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyLeNet5(\n",
              "  (features): Sequential(\n",
              "    (0): FixHWConv2d(32, 32, 1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): FixHWConv2d(14, 14, 6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIPPBdfPN63p",
        "colab_type": "code",
        "outputId": "323c9a6e-f485-4e17-8022-3ccf1adc8a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        " # for energy estimate\n",
        "print('================model energy summary================')\n",
        "energy_info = build_energy_info(model)\n",
        "energy_estimator = lambda m: sum(energy_eval2(m, energy_info, verbose=False).values())\n",
        "energy_estimator_relaxed = lambda m: sum(energy_eval2_relax(m, energy_info, verbose=False).values())\n",
        "\n",
        "reset_Xenergy_cache(energy_info)\n",
        "cur_energy = sum(energy_eval2(model, energy_info, verbose=True).values())\n",
        "cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "\n",
        "dense_model = fill_model_weights(copy.deepcopy(model), 1.0)\n",
        "budget_ub = energy_estimator_relaxed(dense_model)\n",
        "zero_model = fill_model_weights(copy.deepcopy(model), 0.0)\n",
        "budget_lb = energy_estimator_relaxed(zero_model)\n",
        "\n",
        "del zero_model, dense_model\n",
        "budget = max(budget, budget_lb / budget_ub)\n",
        "\n",
        "proj_func = lambda m, budget, grad=False, in_place=True: energy_proj2(m, energy_info, budget, grad=grad,\n",
        "                                                                         in_place=in_place, param_name='weight')\n",
        "print('energy on dense DNN:{:.4e}, on zero DNN:{:.4e}, normalized_lb={:.4e}'.format(budget_ub, budget_lb,\n",
        "                                                                                        budget_lb / budget_ub))\n",
        "print('energy on current DNN:{:.4e}, normalized={:.4e}'.format(cur_energy, cur_energy / budget_ub))\n",
        "print('====================================================')\n",
        "print('current energy {:.4e}, relaxed: {:.4e}'.format(cur_energy, cur_energy_relaxed))\n",
        "netl2wd = l2wd"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================model energy summary================\n",
            "Layer: features.0, input shape: (1, 32, 32), output shape: (6, 28, 28), weight shape: torch.Size([6, 1, 5, 5])\n",
            "Layer: features.0, W_energy=2.07e+05, C_energy=3.53e+05, X_energy=1.38e+06\n",
            "Layer: features.3, input shape: (6, 14, 14), output shape: (16, 10, 10), weight shape: torch.Size([16, 6, 5, 5])\n",
            "Layer: features.3, W_energy=8.50e+05, C_energy=7.20e+05, X_energy=9.75e+05\n",
            "Layer: classifier.0, W_energy=9.94e+06, C_energy=1.44e+05, X_energy=1.74e+05\n",
            "Layer: classifier.2, W_energy=2.09e+06, C_energy=3.02e+04, X_energy=5.52e+04\n",
            "Layer: classifier.4, W_energy=1.74e+05, C_energy=2.52e+03, X_energy=2.01e+04\n",
            "energy on dense DNN:1.7108e+07, on zero DNN:2.6049e+06, normalized_lb=1.5227e-01\n",
            "energy on current DNN:1.7108e+07, normalized=1.0000e+00\n",
            "====================================================\n",
            "current energy 1.7108e+07, relaxed: 1.7108e+07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DPnKghfOsXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if cuda:\n",
        "  if distill > 0.0:\n",
        "    teacher_model.cuda()\n",
        "model.cuda()\n",
        "loss_func = lambda m, x, y: joint_loss(model=m, data=x, target=y, teacher_model=teacher_model, distill=distill)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AjAdtyVPLUL",
        "colab_type": "code",
        "outputId": "28977aed-cddd-471b-bc4d-6a13610ba24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if eval or dataset != 'imagenet':\n",
        "  val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "  print('**Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(val_loss, val_acc1,\n",
        "                                                                                              val_acc5))\n",
        "# also evaluate training data\n",
        "  tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "  print('###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1, tr_acc5))\n",
        "else:\n",
        "    val_acc1 = 0.0\n",
        "    print('For imagenet, skip the first validation evaluation.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**Validation loss:2.3018e+00, top-1 accuracy:0.05770, top-5 accuracy:0.49600\n",
            "###Training loss:2.3023e+00, top-1 accuracy:0.06018, top-5 accuracy:0.49382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5YYKC3iPltD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_file = None\n",
        "\n",
        "energy_step = math.ceil(max(0.0, cur_energy - budget * budget_ub) / ((len(tr_loader) * epochs) / proj_int))\n",
        "\n",
        "energy_decay_factor = min(1.0, (budget * budget_ub) / cur_energy) ** \\\n",
        "                          (1.0 / ((len(tr_loader) * epochs) / proj_int))\n",
        "\n",
        "optimizer = torch.optim.SGD(filtered_parameters(model, param_name='input_mask', inverse=True), lr=0.1, momentum=0.9, weight_decay=netl2wd)\n",
        "if input_mask:\n",
        "  Xoptimizer = torch.optim.Adam(filtered_parameters(model, param_name='input_mask', inverse=False), lr=0.01, weight_decay=xl2wd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQQYzP1ZQSTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cur_budget = cur_energy_relaxed\n",
        "lr = lr_m\n",
        "xlr = xlr\n",
        "cur_sparsity = model_sparsity(model)\n",
        "\n",
        "best_acc_pruned = None\n",
        "Xbudget = 0.9\n",
        "iter_idx = 0\n",
        "\n",
        "W_proj_time = 0.0\n",
        "W_proj_time_cnt = 1e-15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40zehJ4aRi9K",
        "colab_type": "code",
        "outputId": "e6b94fbf-2d54-4750-ed38-51130b85a3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while True:\n",
        "        # update W\n",
        "        if not (skip1 and iter_idx == 0):\n",
        "            t_begin = time.time()\n",
        "            log_tic = t_begin\n",
        "            for epoch in range(epochs):\n",
        "                for batch_idx, (data, target) in enumerate(tr_loader):\n",
        "                    model.train()\n",
        "                    if cuda:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    loss = loss_func(model, data, target)\n",
        "                    # update network weights\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if proj_int == 1 or (batch_idx > 0 and batch_idx % proj_int == 0) or batch_idx == len(tr_loader) - 1:\n",
        "                        temp_tic = time.time()\n",
        "                        proj_func(model, cur_budget)\n",
        "                        W_proj_time += time.time() - temp_tic\n",
        "                        W_proj_time_cnt += 1\n",
        "                        if epoch == epochs - 1 and batch_idx >= len(tr_loader) - 1 - proj_int:\n",
        "                            cur_budget = budget * budget_ub\n",
        "                        else:\n",
        "                            if exp_bdecay:\n",
        "                                cur_budget = max(cur_budget * energy_decay_factor, budget * budget_ub)\n",
        "                            else:\n",
        "                                cur_budget = max(cur_budget - energy_step, budget * budget_ub)\n",
        "                    #print(batch_idx)\n",
        "\n",
        "                    if batch_idx % log_interval == 0:\n",
        "                        print('======================================================')\n",
        "                        print('+-------------- epoch {}, batch {}/{} ----------------+'.format(epoch, batch_idx,\n",
        "                                                                                               len(tr_loader)))\n",
        "                        log_toc = time.time()\n",
        "                        print(\n",
        "                            'primal update: net loss={:.4e}, lr={:.4e}, current normalized budget: {:.4e}, time_elapsed={:.3f}s, averaged projection_time {}'.format(\n",
        "                                loss.item(), optimizer.param_groups[0]['lr'], cur_budget / budget_ub, log_toc - log_tic, W_proj_time / W_proj_time_cnt))\n",
        "                        log_tic = time.time()\n",
        "                        if batch_idx % proj_int == 0:\n",
        "                            cur_sparsity = model_sparsity(model)\n",
        "                        print('sparsity:{}'.format(cur_sparsity))\n",
        "                        print(layers_stat(model, param_names='weight', param_filter=lambda p: p.dim() > 1))\n",
        "                        print('+-----------------------------------------------------+')\n",
        "\n",
        "                cur_energy = energy_estimator(model)\n",
        "                cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "                cur_sparsity = model_sparsity(model)\n",
        "                if epoch % test_interval == 0:\n",
        "                    val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "\n",
        "                    # also evaluate training data\n",
        "                    tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "                    print('###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1,\n",
        "                                                                                                         tr_acc5))\n",
        "\n",
        "                    print(\n",
        "                        '***Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}, current normalized energy:{:.4e}, {:.4e}(relaxed), sparsity: {:.4e}'.format(\n",
        "                            val_loss, val_acc1,\n",
        "                            val_acc5, cur_energy / budget_ub, cur_energy_relaxed / budget_ub, cur_sparsity))\n",
        "                    # save current model\n",
        "                    model_snapshot(model, os.path.join(logdir, 'primal_model_latest.pkl'))\n",
        "\n",
        "                if save_interval > 0 and epoch % save_interval == 0:\n",
        "                    model_snapshot(model, os.path.join(logdir, 'Wprimal_model_epoch{}_{}.pkl'.format(iter_idx, epoch)))\n",
        "\n",
        "                elapse_time = time.time() - t_begin\n",
        "                speed_epoch = elapse_time / (1 + epoch)\n",
        "                eta = speed_epoch * (epochs - epoch)\n",
        "                print(\"Updating Weights, Elapsed {:.2f}s, ets {:.2f}s\".format(elapse_time, eta))\n",
        "\n",
        "        if not input_mask:\n",
        "            print(\"Complete weights training.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Continue to train input mask.\")\n",
        "\n",
        "        if best_acc_pruned is not None and val_acc1 <= best_acc_pruned:\n",
        "            print(\"Pruned accuracy does not improve, stop here!\")\n",
        "            break\n",
        "        best_acc_pruned = val_acc1\n",
        "\n",
        "        # update X\n",
        "        t_begin = time.time()\n",
        "        log_tic = t_begin\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, (data, target) in enumerate(tr_loader):\n",
        "                model.train()\n",
        "                Xoptimizer.param_groups[0]['lr'] = xlr\n",
        "                if cuda:\n",
        "                    data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                loss = loss_func(model, data, target)\n",
        "                # update network weights\n",
        "                Xoptimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                Xoptimizer.step()\n",
        "                clamp_model_weights(model, min=0.0, max=1.0, param_name='input_mask')\n",
        "\n",
        "                if (batch_idx > 0 and batch_idx % proj_int == 0) or batch_idx == len(tr_loader) - 1:\n",
        "                    l0proj(model, Xbudget, param_name='input_mask')\n",
        "\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    print('======================================================')\n",
        "                    print('+-------------- epoch {}, batch {}/{} ----------------+'.format(epoch, batch_idx,\n",
        "                                                                                           len(tr_loader)))\n",
        "                    log_toc = time.time()\n",
        "                    print('primal update: net loss={:.4e}, xlr={:.4e}, time_elapsed={:.3f}s'.format(\n",
        "                            loss.item(), Xoptimizer.param_groups[0]['lr'], log_toc - log_tic))\n",
        "                    log_tic = time.time()\n",
        "                    if batch_idx % proj_int == 0:\n",
        "                        cur_sparsity = model_sparsity(model, param_name='input_mask')\n",
        "                    print('sparsity:{}'.format(cur_sparsity))\n",
        "                    print(layers_stat(model, param_names='input_mask'))\n",
        "                    print('+-----------------------------------------------------+')\n",
        "\n",
        "            cur_energy = energy_estimator(model)\n",
        "            cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "            cur_sparsity = model_sparsity(model, param_name='input_mask')\n",
        "            if epoch % test_interval == 0:\n",
        "\n",
        "                val_loss, val_acc1, val_acc5 = eval_loss_acc1_acc5(model, val_loader, loss_func, cuda)\n",
        "\n",
        "                # also evaluate training data\n",
        "                tr_loss, tr_acc1, tr_acc5 = eval_loss_acc1_acc5(model, train_loader4eval, loss_func, cuda)\n",
        "                print(\n",
        "                    '###Training loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}'.format(tr_loss, tr_acc1,\n",
        "                                                                                                   tr_acc5))\n",
        "\n",
        "                print(\n",
        "                    '***Validation loss:{:.4e}, top-1 accuracy:{:.5f}, top-5 accuracy:{:.5f}, current normalized energy:{:.4e}, {:.4e}(relaxed), sparsity: {:.4e}'.format(\n",
        "                        val_loss, val_acc1,\n",
        "                        val_acc5, cur_energy / budget_ub, cur_energy_relaxed / budget_ub, cur_sparsity))\n",
        "                # save current model\n",
        "                model_snapshot(model, os.path.join(logdir, 'primal_model_latest.pkl'))\n",
        "\n",
        "            if save_interval > 0 and epoch % save_interval == 0:\n",
        "                model_snapshot(model, os.path.join(logdir, 'Xprimal_model_epoch{}_{}.pkl'.format(iter_idx, epoch)))\n",
        "\n",
        "            elapse_time = time.time() - t_begin\n",
        "            speed_epoch = elapse_time / (1 + epoch)\n",
        "            eta = speed_epoch * (epochs - epoch)\n",
        "            print(\"Updating input mask, Elapsed {:.2f}s, ets {:.2f}s\".format(elapse_time, eta))\n",
        "\n",
        "        round_model_weights(model, param_name='input_mask')\n",
        "        # refresh X_energy_cache\n",
        "        reset_Xenergy_cache(energy_info)\n",
        "        cur_energy = energy_estimator(model)\n",
        "        cur_energy_relaxed = energy_estimator_relaxed(model)\n",
        "\n",
        "        iter_idx += 1\n",
        "        Xbudget -= 0.1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=2.3072e+00, lr=1.0000e-01, current normalized budget: 1.0000e+00, time_elapsed=0.292s, averaged projection_time 0.0\n",
            "sparsity:1.0\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=2.5658e-03, mean=9.8756e-02, max=1.9834e-01, nnz=1.0000\n",
            "          features.3abs(W): min=8.0428e-06, mean=4.0750e-02, max=8.1883e-02, nnz=1.0000\n",
            "        classifier.0abs(W): min=3.6417e-07, mean=2.5031e-02, max=5.0461e-02, nnz=1.0000\n",
            "        classifier.2abs(W): min=2.4086e-06, mean=4.6014e-02, max=9.1304e-02, nnz=1.0000\n",
            "        classifier.4abs(W): min=2.2155e-05, mean=5.2406e-02, max=1.0885e-01, nnz=1.0000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=1.0146e+00, lr=1.0000e-01, current normalized budget: 8.4233e-01, time_elapsed=1.175s, averaged projection_time 0.0028511524200439454\n",
            "sparsity:0.8285667805433545\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.8004e-01, max=7.3382e-01, nnz=0.8067\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.2399e-01, max=6.3636e-01, nnz=0.8750\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.4686e-02, max=7.2246e-01, nnz=0.8120\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=7.0748e-02, max=5.5932e-01, nnz=0.8889\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.5500e-01, max=1.4988e+00, nnz=0.9250\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=9.9464e-01, lr=1.0000e-01, current normalized budget: 7.0952e-01, time_elapsed=1.035s, averaged projection_time 0.002873134613037109\n",
            "sparsity:0.6615910200097609\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.4075e-01, max=1.0258e+00, nnz=0.7200\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.4764e-01, max=9.9370e-01, nnz=0.7717\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.9264e-02, max=8.8642e-01, nnz=0.6270\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=8.1797e-02, max=7.2258e-01, nnz=0.7829\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.5464e-01, max=1.4839e+00, nnz=0.8583\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=5.3925e-01, lr=1.0000e-01, current normalized budget: 5.9765e-01, time_elapsed=1.053s, averaged projection_time 0.0029265880584716797\n",
            "sparsity:0.523556206279486\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.5771e-01, max=1.2450e+00, nnz=0.5800\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.5712e-01, max=1.0024e+00, nnz=0.6858\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.7223e-02, max=1.0505e+00, nnz=0.4751\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=8.2775e-02, max=7.3984e-01, nnz=0.6929\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.4696e-01, max=1.4702e+00, nnz=0.7857\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=4.1082e-01, lr=1.0000e-01, current normalized budget: 5.0342e-01, time_elapsed=1.055s, averaged projection_time 0.0027856707572937013\n",
            "sparsity:0.40764600618187735\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.3408e-01, max=1.3825e+00, nnz=0.4667\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.5732e-01, max=1.2712e+00, nnz=0.6075\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=4.3139e-02, max=1.0897e+00, nnz=0.3494\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=8.0919e-02, max=9.6163e-01, nnz=0.6108\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.4215e-01, max=1.4556e+00, nnz=0.7143\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=2.0877e-01, lr=1.0000e-01, current normalized budget: 4.2405e-01, time_elapsed=1.055s, averaged projection_time 0.0026384687423706057\n",
            "sparsity:0.31083455344070277\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.5815e-01, max=1.2319e+00, nnz=0.4133\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.5408e-01, max=1.3523e+00, nnz=0.5104\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=3.9462e-02, max=1.1873e+00, nnz=0.2538\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=7.7695e-02, max=1.0203e+00, nnz=0.5071\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.3726e-01, max=1.4411e+00, nnz=0.6262\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=1.0356e-01, lr=1.0000e-01, current normalized budget: 3.5719e-01, time_elapsed=1.046s, averaged projection_time 0.0025275985399882\n",
            "sparsity:0.23359362290548236\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=2.1988e-01, max=1.2677e+00, nnz=0.3067\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.3921e-01, max=1.5748e+00, nnz=0.4071\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=3.4787e-02, max=1.2519e+00, nnz=0.1866\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=6.9469e-02, max=1.0180e+00, nnz=0.3897\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.3020e-01, max=1.4273e+00, nnz=0.5357\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=7.4922e-01, lr=1.0000e-01, current normalized budget: 3.0087e-01, time_elapsed=1.021s, averaged projection_time 0.0025583880288260323\n",
            "sparsity:0.16933463478119407\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.8812e-01, max=1.4418e+00, nnz=0.2267\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.2162e-01, max=1.7580e+00, nnz=0.3054\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=2.9925e-02, max=1.3043e+00, nnz=0.1331\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=6.0066e-02, max=1.0079e+00, nnz=0.2854\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.2402e-01, max=1.4131e+00, nnz=0.4476\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=1.1119e-01, lr=1.0000e-01, current normalized budget: 2.5343e-01, time_elapsed=0.988s, averaged projection_time 0.0025930523872375487\n",
            "sparsity:0.11464128843338214\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.7188e-01, max=1.4470e+00, nnz=0.1867\n",
            "          features.3abs(W): min=0.0000e+00, mean=1.0207e-01, max=1.7585e+00, nnz=0.2125\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=2.4505e-02, max=1.3272e+00, nnz=0.0885\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=4.8716e-02, max=9.9783e-01, nnz=0.1938\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=1.1224e-01, max=1.3921e+00, nnz=0.3667\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=2.2965e-01, lr=1.0000e-01, current normalized budget: 2.1347e-01, time_elapsed=1.019s, averaged projection_time 0.0025868892669677736\n",
            "sparsity:0.07102651700016269\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.2092e-01, max=1.6164e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.8084e-02, max=1.8076e+00, nnz=0.1317\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.9013e-02, max=1.4532e+00, nnz=0.0550\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.5939e-02, max=9.8845e-01, nnz=0.1161\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=9.5775e-02, max=1.3782e+00, nnz=0.2679\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:2.5714e-01, top-1 accuracy:0.93350, top-5 accuracy:0.99238\n",
            "***Validation loss:2.5595e-01, top-1 accuracy:0.93400, top-5 accuracy:0.99150, current normalized energy:1.9996e-01, 1.9996e-01(relaxed), sparsity: 5.1440e-02\n",
            "Updating Weights, Elapsed 20.92s, ets 20.92s\n",
            "Continue to train input mask.\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=3.8210e-01, xlr=1.0000e-04, time_elapsed=0.259s\n",
            "sparsity:1.0\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=9.9990e-01, mean=9.9994e-01, max=1.0000e+00, nnz=1.0000\n",
            "          features.3abs(W): min=9.9990e-01, mean=9.9992e-01, max=1.0000e+00, nnz=1.0000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=2.3427e-01, xlr=1.0000e-04, time_elapsed=1.081s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1322e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.8282e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=5.4373e-01, xlr=1.0000e-04, time_elapsed=0.983s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1259e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.7818e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=2.0926e-01, xlr=1.0000e-04, time_elapsed=0.987s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1206e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.7365e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=2.0318e-01, xlr=1.0000e-04, time_elapsed=0.964s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1154e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.6920e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=2.3403e-01, xlr=1.0000e-04, time_elapsed=1.018s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1090e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.6473e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=2.8439e-01, xlr=1.0000e-04, time_elapsed=0.998s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1038e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.6030e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=3.9596e-01, xlr=1.0000e-04, time_elapsed=0.992s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0982e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.5591e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=1.5943e-01, xlr=1.0000e-04, time_elapsed=1.001s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0932e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.5156e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=8.4372e-02, xlr=1.0000e-04, time_elapsed=0.995s\n",
            "sparsity:0.9\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0884e-01, max=1.0000e+00, nnz=0.9141\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.4722e-01, max=1.0000e+00, nnz=0.8878\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:2.4970e-01, top-1 accuracy:0.93598, top-5 accuracy:0.99280\n",
            "***Validation loss:2.5027e-01, top-1 accuracy:0.93680, top-5 accuracy:0.99160, current normalized energy:1.9334e-01, 1.9416e-01(relaxed), sparsity: 9.0000e-01\n",
            "Updating input mask, Elapsed 20.49s, ets 20.49s\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=1.7154e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=0.265s, averaged projection_time 0.002548732656113645\n",
            "sparsity:0.9787538636733366\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=3.8399e-35, mean=1.1526e-01, max=1.7478e+00, nnz=1.0000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.6647e-02, max=1.8342e+00, nnz=0.9992\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.5991e-02, max=1.4752e+00, nnz=0.9738\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.8597e-02, max=9.8470e-01, nnz=0.9955\n",
            "        classifier.4abs(W): min=1.1069e-24, mean=8.0912e-02, max=1.3730e+00, nnz=1.0000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=3.0710e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.099s, averaged projection_time 0.0025386856152461125\n",
            "sparsity:0.05692207580933789\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3175e-01, max=1.8884e+00, nnz=0.1200\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8695e-02, max=1.7600e+00, nnz=0.1100\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6266e-02, max=1.4036e+00, nnz=0.0436\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=2.9463e-02, max=9.7490e-01, nnz=0.0911\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4267e-02, max=1.3593e+00, nnz=0.2464\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=2.5779e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.013s, averaged projection_time 0.002585996661269874\n",
            "sparsity:0.057296242069302096\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3003e-01, max=2.0490e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9347e-02, max=1.8166e+00, nnz=0.1121\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6421e-02, max=1.4753e+00, nnz=0.0434\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0293e-02, max=1.1711e+00, nnz=0.0935\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4891e-02, max=1.3458e+00, nnz=0.2500\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=2.7456e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.044s, averaged projection_time 0.0025489503337490942\n",
            "sparsity:0.057556531641451115\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3426e-01, max=2.3017e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9304e-02, max=1.9820e+00, nnz=0.1087\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6479e-02, max=1.4627e+00, nnz=0.0437\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0206e-02, max=9.5557e-01, nnz=0.0947\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.3228e-02, max=1.3324e+00, nnz=0.2500\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=3.8522e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.015s, averaged projection_time 0.0025569591949235145\n",
            "sparsity:0.0577842850170815\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3173e-01, max=2.4958e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8660e-02, max=1.9801e+00, nnz=0.1062\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6480e-02, max=1.4786e+00, nnz=0.0437\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0537e-02, max=9.5108e-01, nnz=0.0961\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5013e-02, max=1.3191e+00, nnz=0.2560\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=1.4786e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.090s, averaged projection_time 0.0025514745050006444\n",
            "sparsity:0.05775174882056287\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3286e-01, max=2.4382e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9002e-02, max=1.9031e+00, nnz=0.1067\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6476e-02, max=1.5230e+00, nnz=0.0437\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0318e-02, max=9.4150e-01, nnz=0.0955\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5573e-02, max=1.3059e+00, nnz=0.2607\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=3.1036e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.042s, averaged projection_time 0.002513181079517711\n",
            "sparsity:0.057849357410118755\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3215e-01, max=2.4363e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.7997e-02, max=1.8714e+00, nnz=0.1054\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6453e-02, max=1.5964e+00, nnz=0.0435\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0324e-02, max=9.3213e-01, nnz=0.0967\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6695e-02, max=1.2929e+00, nnz=0.2655\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=1.0859e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.031s, averaged projection_time 0.002496377723972972\n",
            "sparsity:0.057849357410118755\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3187e-01, max=2.3670e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.7771e-02, max=1.8887e+00, nnz=0.1054\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6440e-02, max=1.6202e+00, nnz=0.0436\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0369e-02, max=9.2284e-01, nnz=0.0964\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6580e-02, max=1.2801e+00, nnz=0.2643\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=1.1640e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.092s, averaged projection_time 0.0025844382143568718\n",
            "sparsity:0.057702944525784935\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.4016e-01, max=2.4635e+00, nnz=0.1133\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.7530e-02, max=1.9410e+00, nnz=0.1037\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6482e-02, max=1.5896e+00, nnz=0.0435\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0406e-02, max=1.0105e+00, nnz=0.0971\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4677e-02, max=1.2673e+00, nnz=0.2536\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=1.5517e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.029s, averaged projection_time 0.0025647256685339885\n",
            "sparsity:0.05833740035789816\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3183e-01, max=2.5431e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.7216e-02, max=1.9792e+00, nnz=0.1029\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6585e-02, max=1.6212e+00, nnz=0.0439\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0592e-02, max=9.9696e-01, nnz=0.0988\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.4913e-02, max=1.2547e+00, nnz=0.2619\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:1.4528e-01, top-1 accuracy:0.96215, top-5 accuracy:0.99687\n",
            "***Validation loss:1.4390e-01, top-1 accuracy:0.96120, top-5 accuracy:0.99690, current normalized energy:1.9914e-01, 1.9998e-01(relaxed), sparsity: 5.8224e-02\n",
            "Updating Weights, Elapsed 21.08s, ets 21.08s\n",
            "Continue to train input mask.\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=5.1840e-02, xlr=1.0000e-04, time_elapsed=0.278s\n",
            "sparsity:0.9118181818181819\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1406e-01, max=1.0000e+00, nnz=0.9385\n",
            "          features.3abs(W): min=0.0000e+00, mean=8.8771e-01, max=1.0000e+00, nnz=0.8886\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=1.9401e-02, xlr=1.0000e-04, time_elapsed=1.094s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1232e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9864e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=1.0304e-01, xlr=1.0000e-04, time_elapsed=0.986s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1178e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9592e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=1.3986e-01, xlr=1.0000e-04, time_elapsed=1.011s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1121e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9324e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=8.0380e-02, xlr=1.0000e-04, time_elapsed=1.034s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1062e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9053e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=2.0645e-01, xlr=1.0000e-04, time_elapsed=1.034s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.1006e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8792e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=1.0187e-01, xlr=1.0000e-04, time_elapsed=1.001s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0951e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8532e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=3.7385e-01, xlr=1.0000e-04, time_elapsed=0.974s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0887e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8272e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=7.8933e-02, xlr=1.0000e-04, time_elapsed=0.995s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0842e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8022e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=1.9668e-01, xlr=1.0000e-04, time_elapsed=1.001s\n",
            "sparsity:0.8\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=9.0801e-01, max=1.0000e+00, nnz=0.9131\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.7773e-01, max=1.0000e+00, nnz=0.7015\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:1.3804e-01, top-1 accuracy:0.96380, top-5 accuracy:0.99697\n",
            "***Validation loss:1.3676e-01, top-1 accuracy:0.96290, top-5 accuracy:0.99690, current normalized energy:1.9180e-01, 1.9323e-01(relaxed), sparsity: 8.0000e-01\n",
            "Updating input mask, Elapsed 20.92s, ets 20.92s\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 0/938 ----------------+\n",
            "primal update: net loss=6.2480e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=0.266s, averaged projection_time 0.0025445476491400536\n",
            "sparsity:0.2758581421831788\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3575e-01, max=2.6481e+00, nnz=0.5000\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.7570e-02, max=2.0088e+00, nnz=0.2292\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.6615e-02, max=1.6166e+00, nnz=0.2316\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.0591e-02, max=9.8897e-01, nnz=0.4533\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5652e-02, max=1.2499e+00, nnz=0.7702\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 100/938 ----------------+\n",
            "primal update: net loss=2.6411e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.099s, averaged projection_time 0.002547697587446733\n",
            "sparsity:0.06593460224499756\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.4112e-01, max=2.6400e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.0172e-02, max=1.9034e+00, nnz=0.1183\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.7328e-02, max=1.6535e+00, nnz=0.0500\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.1408e-02, max=9.5093e-01, nnz=0.1095\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7291e-02, max=1.2375e+00, nnz=0.3000\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 200/938 ----------------+\n",
            "primal update: net loss=1.4078e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.039s, averaged projection_time 0.002563765415778527\n",
            "sparsity:0.06614608752236864\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3086e-01, max=2.6084e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.0545e-02, max=1.9801e+00, nnz=0.1167\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.7501e-02, max=1.6620e+00, nnz=0.0505\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.1691e-02, max=9.6275e-01, nnz=0.1098\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7032e-02, max=1.2252e+00, nnz=0.2869\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 300/938 ----------------+\n",
            "primal update: net loss=5.8473e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.032s, averaged projection_time 0.002571019557637906\n",
            "sparsity:0.06627623230844314\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3559e-01, max=2.5307e+00, nnz=0.1000\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.0223e-02, max=1.9289e+00, nnz=0.1150\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.7760e-02, max=1.7156e+00, nnz=0.0507\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.1827e-02, max=9.4239e-01, nnz=0.1102\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.8259e-02, max=1.2130e+00, nnz=0.2857\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 400/938 ----------------+\n",
            "primal update: net loss=1.6597e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.020s, averaged projection_time 0.002550159630022551\n",
            "sparsity:0.06652025378233284\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.4088e-01, max=2.6370e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8305e-02, max=1.8382e+00, nnz=0.1087\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.7914e-02, max=1.6472e+00, nnz=0.0509\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.2075e-02, max=1.0504e+00, nnz=0.1120\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.9379e-02, max=1.2009e+00, nnz=0.2869\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 500/938 ----------------+\n",
            "primal update: net loss=3.1170e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.007s, averaged projection_time 0.0025290891903789105\n",
            "sparsity:0.06652025378233284\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.4219e-01, max=2.6011e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8475e-02, max=1.8973e+00, nnz=0.1087\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.8103e-02, max=1.6143e+00, nnz=0.0506\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.2089e-02, max=1.0308e+00, nnz=0.1127\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.8936e-02, max=1.1889e+00, nnz=0.2929\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 600/938 ----------------+\n",
            "primal update: net loss=1.2767e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.025s, averaged projection_time 0.0025051120788820327\n",
            "sparsity:0.06642264519277696\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3732e-01, max=2.6211e+00, nnz=0.1067\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.8787e-02, max=1.9832e+00, nnz=0.1096\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.8111e-02, max=1.5922e+00, nnz=0.0505\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.2097e-02, max=1.0199e+00, nnz=0.1129\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.8199e-02, max=1.1771e+00, nnz=0.2893\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 700/938 ----------------+\n",
            "primal update: net loss=9.2173e-02, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.077s, averaged projection_time 0.0025125520173893416\n",
            "sparsity:0.06625996421018383\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.3562e-01, max=2.7369e+00, nnz=0.1133\n",
            "          features.3abs(W): min=0.0000e+00, mean=6.9293e-02, max=1.9894e+00, nnz=0.1087\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.8207e-02, max=1.6462e+00, nnz=0.0502\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.2120e-02, max=1.0096e+00, nnz=0.1128\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.7897e-02, max=1.1653e+00, nnz=0.2952\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 800/938 ----------------+\n",
            "primal update: net loss=5.2876e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.084s, averaged projection_time 0.002500985985371604\n",
            "sparsity:0.06450300959817798\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.5363e-01, max=2.1632e+00, nnz=0.1400\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.4804e-02, max=2.1050e+00, nnz=0.1167\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.8378e-02, max=1.6306e+00, nnz=0.0484\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.2569e-02, max=9.9953e-01, nnz=0.1105\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.6307e-02, max=1.1537e+00, nnz=0.2726\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "======================================================\n",
            "+-------------- epoch 0, batch 900/938 ----------------+\n",
            "primal update: net loss=1.5132e-01, lr=1.0000e-01, current normalized budget: 2.0000e-01, time_elapsed=1.045s, averaged projection_time 0.0024882640770013385\n",
            "sparsity:0.06536521880592158\n",
            "########### layer stat ###########\n",
            "          features.0abs(W): min=0.0000e+00, mean=1.4514e-01, max=2.2104e+00, nnz=0.1267\n",
            "          features.3abs(W): min=0.0000e+00, mean=7.4542e-02, max=2.1717e+00, nnz=0.1129\n",
            "        classifier.0abs(W): min=0.0000e+00, mean=1.8713e-02, max=1.6568e+00, nnz=0.0493\n",
            "        classifier.2abs(W): min=0.0000e+00, mean=3.3037e-02, max=9.8958e-01, nnz=0.1123\n",
            "        classifier.4abs(W): min=0.0000e+00, mean=8.5455e-02, max=1.1423e+00, nnz=0.2762\n",
            "########### layer stat ###########\n",
            "+-----------------------------------------------------+\n",
            "###Training loss:2.1427e-01, top-1 accuracy:0.94922, top-5 accuracy:0.99447\n",
            "***Validation loss:2.1024e-01, top-1 accuracy:0.95270, top-5 accuracy:0.99380, current normalized energy:1.9853e-01, 1.9998e-01(relaxed), sparsity: 6.5560e-02\n",
            "Updating Weights, Elapsed 21.27s, ets 21.27s\n",
            "Continue to train input mask.\n",
            "Pruned accuracy does not improve, stop here!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUJ6ERWIZTiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filtered_parameters_name(model, param_name, inverse=False):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(name)\n",
        "        if inverse != (name.endswith(param_name)):\n",
        "            yield param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDM2FM2KZ41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = filtered_parameters(model, param_name='input_mask', inverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luo5GnXmKeEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6b13130-5fe2-497e-fd53-493c8c030fb8"
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name)\n",
        "  if True != (name.endswith('input_mask')):\n",
        "    print(param)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features.0.weight\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.6693,  0.0000,  0.0000, -1.0239,  0.0000],\n",
            "          [ 0.0000, -1.3391, -1.1952,  0.0000,  0.0000],\n",
            "          [ 0.0000, -1.1915,  0.0000,  0.0000,  0.0000],\n",
            "          [-1.6192,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -1.0433,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.5006,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.4829, -0.4241,  0.0000,  1.3034],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.8164,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.9133,  0.0000,  0.0000, -0.7953, -2.1647]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -2.2252,  0.0000],\n",
            "          [-1.9256,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  1.5002]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "features.0.bias\n",
            "Parameter containing:\n",
            "tensor([-0.8747, -2.3758, -3.4771, -2.6914, -1.9853, -1.9419], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "features.0.xi\n",
            "Parameter containing:\n",
            "tensor([1], device='cuda:0')\n",
            "features.0.g\n",
            "Parameter containing:\n",
            "tensor([1], device='cuda:0')\n",
            "features.0.p\n",
            "Parameter containing:\n",
            "tensor([0], device='cuda:0')\n",
            "features.0.input_mask\n",
            "features.3.weight\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.3893,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.3557,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.3610,  0.0000,  0.0000,  0.0000, -0.5318],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4412],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.6948],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5263],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.3726,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.4358]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.3602,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000, -0.3582,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.3884],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1585,  0.1700,  0.0000,  0.0000, -0.3648],\n",
            "          [ 0.9520,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  1.1472,  0.0000,  0.0000],\n",
            "          [ 0.3759,  0.0000,  0.0000, -0.3107, -0.4986],\n",
            "          [ 0.0000, -1.0022, -1.5046, -0.9910, -0.8147]],\n",
            "\n",
            "         [[-0.4741, -0.3670, -0.3642, -0.3697,  0.0000],\n",
            "          [-0.3591,  0.0000,  0.0000, -0.4058, -0.3874],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.3421,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.3800, -0.3874, -0.4899, -0.3773]],\n",
            "\n",
            "         [[ 0.0000, -0.2903, -0.3625, -0.4527, -0.3309],\n",
            "          [ 0.0000, -0.1518, -0.3542,  0.0000, -0.4957],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [-0.1244, -0.4381, -0.5660, -0.7648, -0.6457]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-1.3234, -1.2134,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -1.2554, -1.5007, -1.0833, -0.5364],\n",
            "          [ 0.7944,  0.8621,  0.0000,  0.0000,  0.0000],\n",
            "          [ 1.2923,  0.0000,  0.7427,  0.0000,  0.0000],\n",
            "          [-0.3577,  0.0000, -0.7834,  0.0000, -0.3320]],\n",
            "\n",
            "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  1.2180,  1.0426,  1.2706,  0.0000],\n",
            "          [ 0.0000,  0.0000,  1.0801,  0.7820,  0.0000],\n",
            "          [-0.8167, -1.0992, -1.1322, -0.7157,  0.0000],\n",
            "          [-0.4954, -1.3397, -1.4299,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.4613,  0.0000,  0.0000, -1.0299]],\n",
            "\n",
            "         [[ 0.0000, -0.4024, -0.3459,  0.0000,  0.0000],\n",
            "          [-0.4823,  0.0000, -0.6353, -0.5385, -0.5280],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5735],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.4479, -0.5490],\n",
            "          [ 0.0000,  0.0000,  0.4893,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.4403, -0.3392,  0.8351, -0.5148,  0.4290],\n",
            "          [-0.6105,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.7553,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -1.0247, -0.8438]],\n",
            "\n",
            "         [[-0.3458,  0.0000,  0.0000,  0.0000, -0.3851],\n",
            "          [ 0.0000,  0.0000, -0.5208,  0.0000, -0.4219],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5147],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[-0.2095, -1.2288, -1.2571, -0.4724,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.7330, -1.3987, -0.9295],\n",
            "          [ 0.0000,  0.0000,  0.0000, -1.1222, -0.7802],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "         [[ 0.0000,  0.0000, -0.3867,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "features.3.bias\n",
            "Parameter containing:\n",
            "tensor([-9.6988e-02, -1.8152e-01, -6.5295e-02, -1.5901e-01, -9.3883e-02,\n",
            "        -1.7062e-01, -8.6032e-02, -2.4011e-02,  8.6689e-01,  4.1191e-04,\n",
            "        -4.2651e-02, -7.5272e-02,  4.2915e-01, -6.6040e-02, -6.4864e-01,\n",
            "        -5.6447e-01], device='cuda:0', requires_grad=True)\n",
            "features.3.xi\n",
            "Parameter containing:\n",
            "tensor([1], device='cuda:0')\n",
            "features.3.g\n",
            "Parameter containing:\n",
            "tensor([1], device='cuda:0')\n",
            "features.3.p\n",
            "Parameter containing:\n",
            "tensor([0], device='cuda:0')\n",
            "features.3.input_mask\n",
            "classifier.0.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.3746,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "classifier.0.bias\n",
            "Parameter containing:\n",
            "tensor([-0.0442, -0.0341, -0.0377, -0.0441,  0.0099, -0.2225, -0.0289,  0.0024,\n",
            "        -0.0158, -0.0351, -0.0823, -0.0097, -0.0784,  0.0171, -0.0138, -0.0755,\n",
            "        -0.0009, -0.3432, -0.1065,  0.0078,  0.0811, -0.0520,  0.0073,  0.0633,\n",
            "         0.0113, -0.0225, -0.0304, -0.0068, -0.0212, -0.0133,  0.0241, -0.1691,\n",
            "        -0.0212, -0.0870, -0.0280, -0.0084, -0.1172,  0.0290, -0.0420, -0.0190,\n",
            "        -0.0495, -0.0203, -0.0369,  0.7940,  0.0055,  0.0442, -0.2558, -0.0301,\n",
            "        -0.0551,  0.0179,  0.0123, -0.0407, -0.0676,  0.0205, -0.0242, -0.1804,\n",
            "        -0.0233, -0.0360, -0.0547, -0.0352, -0.0019, -0.1170, -0.0046, -0.0418,\n",
            "         0.0078,  0.0220, -0.0247, -0.0975, -0.0291, -0.0197, -0.0906, -0.3571,\n",
            "         0.0376, -0.0073,  0.0910,  0.3819, -0.0244, -0.1448, -0.0662, -0.0262,\n",
            "        -0.0730, -0.0546,  0.0294,  0.1574, -0.0960,  0.0359, -0.0348,  0.0255,\n",
            "        -0.0387, -0.0418, -0.1181, -0.0164, -0.1095, -0.0027, -0.1682,  0.0766,\n",
            "        -0.0023,  0.0959, -0.0022, -0.1100, -0.0225,  0.2420,  0.0312, -0.3237,\n",
            "        -0.0315,  0.1355, -0.0736, -0.0475, -0.0377,  0.0109, -0.1731, -0.0299,\n",
            "        -0.0432,  0.0752, -0.0301,  0.1033, -0.0250, -0.0009, -0.0264,  0.0088],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "classifier.2.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ..., -0.2226,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.1480,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ..., -0.2637,  0.0000,  0.0000]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "classifier.2.bias\n",
            "Parameter containing:\n",
            "tensor([-0.0552, -0.0594, -0.0029,  0.0065, -0.0222,  0.0964, -0.0660, -0.0043,\n",
            "         0.3127,  0.5494,  0.0745,  0.2531, -0.0618,  0.0501, -0.0182,  0.0808,\n",
            "         0.2898, -0.1034, -0.0754, -0.1737,  0.0409, -0.0830, -0.1397, -0.0956,\n",
            "         0.3723,  0.9144, -0.0063,  0.0142,  0.4922,  0.3588, -0.0172, -0.1677,\n",
            "        -0.1039,  0.4378, -0.0931, -0.0405,  0.4081, -0.1976, -0.0870,  0.2349,\n",
            "         0.0212,  0.2278, -0.0931,  0.0679,  0.1122,  0.3475,  0.1416, -0.1720,\n",
            "         0.1331, -0.0130, -0.1689,  0.1886,  0.1461,  0.0957, -0.1138, -0.1412,\n",
            "         0.4559,  0.0234, -0.1853,  0.1882, -0.0017, -0.0918,  0.3504, -0.0821,\n",
            "        -0.0918, -0.1531, -0.1019, -0.0434, -0.1104, -0.1558,  0.1274, -0.0721,\n",
            "         0.1078,  0.5869,  0.0405,  0.0840,  0.1790, -0.1412, -0.0405, -0.1428,\n",
            "         0.0765,  0.0050, -0.1180, -0.0488], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "classifier.4.weight\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000, -0.1708, -0.0819,  0.0000,  0.0000,  0.0000,  0.4253,\n",
            "          0.2235, -0.3242,  0.0000,  0.1261,  0.0000,  0.0000,  0.0000,  0.1205,\n",
            "          0.0000,  0.6404,  0.0000,  0.1663,  0.1267,  0.0000, -0.2504,  0.0000,\n",
            "          0.3924,  0.2236,  0.0000,  0.9779,  0.0000,  0.0000,  0.0000, -0.3141,\n",
            "          0.0000, -0.2732,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.1789,  0.0000,  0.0000,  0.0000, -0.1836, -0.2436,  0.2211,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2668,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2952,  0.0000, -0.1890,\n",
            "          0.2072,  0.0000,  0.0000,  0.1085, -0.2714,  0.2271,  0.0000,  0.0000,\n",
            "          0.0500,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.1823,  0.0000,  0.0000, -0.4277,  0.0000,  0.0000,\n",
            "         -0.5729,  0.0000,  0.0000,  0.3793, -0.3097,  0.2176,  0.0000,  0.2978,\n",
            "          0.0000, -0.2986,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.7054,  0.0000, -0.2751,  0.2512,  0.0000,  0.0000,  0.0000,\n",
            "         -0.3405,  0.0000, -0.2889,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.7241,  0.0000,  0.0000, -0.4487,  0.0000,  0.0000,  0.0000,\n",
            "          0.2166,  0.0000, -0.3188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.1440,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.4011, -0.3514,  0.0000,  0.0000, -0.2040,  0.0000,  0.1962,\n",
            "         -0.1372,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.2006,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.5129,  0.1035,  0.0000,  0.0000,  0.3087,  0.0000,  0.0000,\n",
            "          0.1063,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.2806,  0.0000,  0.0600,  0.1368,  0.1996,\n",
            "          0.0000,  0.0000, -0.3361,  0.0000,  0.0000,  0.1596,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.1721,  0.0000,  0.3947,  0.0000,\n",
            "         -0.2725,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.2657,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1416,  0.0000,  0.0000,\n",
            "          0.5557,  0.2891,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5051,\n",
            "          0.2228,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.2246,  0.0000,  0.0000,  0.0000,  0.0000,  0.2290,  0.0000,  0.0000,\n",
            "          0.2406,  0.1227,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.3742,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.2662,  0.0000,  0.0000,  0.0000,  0.1538,  0.2765,  0.0000,  0.0000,\n",
            "          0.3346,  0.0000,  0.0000,  0.0000, -0.3988, -0.2878,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.1305,  0.0000,  0.0000,  0.3949,  0.0000,\n",
            "          0.0724,  0.0000,  0.0000,  0.0000,  0.0581, -0.0593,  0.0000,  0.0000,\n",
            "          0.5296,  0.0000,  0.0000,  0.0000,  0.0000, -0.2183,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.6133,  0.0000,  0.1692, -0.3448,  0.2588,  0.0000,  0.1679,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5918,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.1565,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -1.1380,  0.3441,  0.0000,  0.2033, -0.2946,  0.0000, -0.3581,\n",
            "         -0.3603, -0.1646, -0.5169,  0.0000, -0.1558,  0.0000,  0.9764,  0.0000,\n",
            "         -0.2793, -0.2869,  0.0000,  0.0000,  0.4500,  0.6180,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0725, -0.5978, -0.5855, -0.2101,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.5597, -0.2807,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.5603,  0.0000,  0.0000, -1.0641,\n",
            "          0.0000,  0.0000,  0.2314,  0.0000,  0.0000,  0.0000, -0.6429,  0.0000,\n",
            "          0.0000, -0.5466,  0.0000,  0.0000,  0.0000, -0.3114,  0.0000,  0.1918,\n",
            "          0.1221,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1749,  0.0000,  0.0000,\n",
            "          0.3682, -0.1815,  0.1881,  0.0000,  0.0000, -0.4855,  0.0000,  0.0000,\n",
            "          0.1729,  0.0000,  0.0000,  0.1983,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000, -0.1920,  0.0892,  0.1431,  0.2581,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000, -0.4095,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0869,  0.0000,\n",
            "          0.0000,  0.0000, -0.4408,  0.0000,  0.2509,  0.3982,  0.0000,  0.2352,\n",
            "          0.0000,  0.0000,  0.0000,  0.1229,  0.0000,  0.0000,  0.2562,  0.0000,\n",
            "          0.0000,  0.0000, -0.2305,  0.0000,  0.2342,  0.0000,  0.0000,  0.0000,\n",
            "         -0.2432,  0.4195,  0.0000,  0.5804,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.5846,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.4737,  0.0000,  0.0000,  0.0000, -0.2575,\n",
            "          0.4062,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.2195, -0.3159, -0.2116, -0.4646,  0.0000,  0.2387,  0.0000,\n",
            "          0.0000,  0.1974,  0.1352,  0.0000,  0.0000,  0.0000,  0.0000,  0.2397,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1416,  0.0000,\n",
            "          0.0000,  0.0945,  0.0000,  0.2971, -0.4063,  0.0000,  0.0000,  0.0000,\n",
            "         -0.5129,  0.0000,  0.0000, -0.4581,  0.1021,  0.0000,  0.0000,  0.0000,\n",
            "         -0.2997,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3032,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5503,  0.0000,  0.2900,\n",
            "          0.0000,  0.0000,  0.0000,  0.0807,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         -0.3453,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.3281,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3334,  0.0000,  0.0000,\n",
            "         -0.2095,  0.2367,  0.3286,  0.1828,  0.2331,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000, -0.1283,  0.0000,  0.0000,  0.0000,  0.0597,  0.3010,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2404,  0.0000, -0.2534,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.1407,  0.0000,  0.1580,  0.2626,\n",
            "          0.2969,  0.0000,  0.0000, -0.1010, -0.1105,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.2068,  0.0000,  0.2713,  0.3396,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.4685, -0.1553,  0.0000,  0.0000,\n",
            "          0.2768,  0.0000,  0.0000, -0.4936,  0.0000,  0.0000,  0.0000,  0.2885,\n",
            "          0.3387,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.1564,  0.0000,  0.0000,  0.0000,  0.0000,  0.1367,  0.0000,  0.0000,\n",
            "          0.3593,  0.1409,  0.5380,  0.2614,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.1303,  0.0000,  0.0000,  0.0000,\n",
            "          0.1553,  0.1992,  0.0000,  0.0000,  0.4502,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.1980,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000, -0.2895,  0.0629,  0.1029,  0.0000,  0.0000,\n",
            "          0.4227,  0.0000,  0.0000,  0.0000,  0.1434,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000, -0.2581,  0.1240,  0.0000,  0.0000,\n",
            "          0.0000,  0.1681,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.1398,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1408,\n",
            "          0.0000, -0.7106,  0.3084,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0929,  0.6148,  0.2909,  0.0000,  0.5632,  0.0000,  0.0000,  0.0000,\n",
            "          0.1554,  0.0000,  0.3710,  0.3092,  0.0000,  0.0000,  0.3610,  0.0000,\n",
            "          0.0000, -0.4665,  0.0000,  0.0000,  0.1779, -0.7178,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.4490,  0.0000,  0.4764,  0.0000, -0.2558,  0.0000,  0.0000,  0.0000,\n",
            "          0.2871,  0.0000,  0.0000,  0.3003,  0.0000,  0.0000,  0.1768,  0.5379,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.1642,  0.0000,  0.0000,  0.0000]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "classifier.4.bias\n",
            "Parameter containing:\n",
            "tensor([-0.5805,  0.1696, -0.3423,  0.3668,  0.5560,  0.7643, -0.7705, -0.2297,\n",
            "         0.1230,  0.2368], device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3jfaHrKk_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}